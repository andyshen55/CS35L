I set up the lab environment with the following shell commands:
locale
export LC_ALL='C'
sort /usr/share/dict/words > words
wget https://web.cs.ucla.edu/classes/fall19/cs35L/assign/assign2.html



I then ran the prescribed shell commands.

cat assign2.html | tr -c 'A-Za-z' '[\n*]': This command outputs the HTML file with all non-alphabetical characters replaced with newlines.

cat assign2.html | tr -cs 'A-Za-z' '[\n*]': The output of this command differed from the previous one in that it squeezed all consecutive newlines into a single newline. This made the output vastly more readable.

cat assign2.html | tr -cs 'A-Za-z' '[\n*]' | sort: The output of this command was the alphabetically sorted version of the previous command.

cat assign2.html | tr -cs 'A-Za-z' '[\n*]' | sort -u: The output of this command was the same as the previous command, except with non-unique instances of words removed.

cat assign2.html | tr -cs 'A-Za-z' '[\n*]' | sort -u | comm - words: The output of this command compares the sorted output of the previous command (sorted-assign)  against the alphabetically sorted list of words in words. It splits the output into three columns: the first displays lines unique to sorted-assign, the second displays lines unique to words, and the third displays lines found in both.

cat assign2.html | tr -cs 'A-Za-z' '[\n*]' | sort -u | comm -23 - words: The output of this command is the same as that of the previous command, except with columns 2 and 3 not being displayed. In other words, this command displays all the lines that are unique to sorted-assign.



After running the shell commands, I had to extract a list of Hawaiian words from the given website.
I ran the command: wget -O hawaiian.html https://www.mauimapp.com/moolelo/hwnwdshw.htm
to get a local copy of the HTML file from the prescribed website.

In order to automate the systematic ruleset prescribed in the assignment specification, I wrote a shell script buildwords.sh

In pseudocode, buildwords.sh matches all lines with enclosing <td> tags, as all the  Hawaiian words are contained within these tags. It then removes all '?', '<ul>', '</ul>' occurences as prescribed by the spec. I then removed the enclosing <td> tags and replaced grave accents with apostrophes. buildwords.sh then separates phrases into individual words and filters out any words that contain letters that aren't present in the Hawaiian alphabet. The remaining words are then sorted alphabetically and pruend for repeat occurences.

emacs buildwords.sh

#! /bin/bash

:' Usage: ./buildwords < input_file(s)
Automates systematic rules to extract Hawaiian words from given URL
'

#Matches lines enclosed in <td> tags
grep -E "<td>.*</td>" $@ |

#Removes tags and ? char
sed 's/(\? | <ul> | <\/ul>)//g' |
sed 's/<td>//g' |
sed "s/<\/td>//g" |

#Replaces ASCII grave accents with apostrophes
sed "s/\`/\'/g" |

#separates phrases into individual words (on separate lines)
tr ',' '\n' |
tr ' ' '\n' |

#truncates extraneous newlines
tr -s '\n' |

#converts to lowercase
tr [:upper:] [:lower:] |

#deletes lines containing non-Hawaiian alphabetical characters
sed "/[^pk\'mnwlhaeiou\n ]/d" |

#filters unique words alphabetically
sort -u

